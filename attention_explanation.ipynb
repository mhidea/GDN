{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Attention Mechanism Explained with PyTorch\n",
                "\n",
                "This notebook provides a simple explanation and implementation of the attention mechanism, specifically Scaled Dot-Product Attention, using PyTorch."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Introduction\n",
                "\n",
                "Attention mechanisms have become a fundamental component in many state-of-the-art deep learning models, particularly in Natural Language Processing (NLP) tasks like machine translation, text summarization, and question answering. \n",
                "\n",
                "The core idea behind attention is to allow the model to dynamically focus on different parts of the input sequence when producing an output. Instead of relying solely on the final hidden state of an encoder (like in traditional sequence-to-sequence models), attention allows the decoder (or subsequent layers) to \"look back\" at the entire input sequence and assign different weights (attention scores) to different input parts based on their relevance to the current output step."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Core Idea: Query, Key, Value\n",
                "\n",
                "Attention can be described in terms of three components:\n",
                "\n",
                "1.  **Query (Q):** Represents the current context or state trying to retrieve information. In a sequence-to-sequence model's decoder, this might be the hidden state of the decoder at the current time step.\n",
                "2.  **Key (K):** Paired with values. Queries are compared against keys to determine attention weights. In a sequence-to-sequence model, keys often correspond to the hidden states of the encoder for each input token.\n",
                "3.  **Value (V):** The actual information associated with the keys. Once attention weights are calculated by comparing the query to the keys, these weights are used to create a weighted sum of the values. Values often correspond to the same source as keys (e.g., encoder hidden states).\n",
                "\n",
                "The goal is: given a query, compute a weighted sum of the values, where the weight assigned to each value is determined by the compatibility (similarity) of the query with its corresponding key."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Scaled Dot-Product Attention\n",
                "\n",
                "This is one of the most common and effective attention mechanisms, popularized by the \"Attention Is All You Need\" paper (Transformer model).\n",
                "\n",
                "The formula is:\n",
                "```\n",
                "Attention(Q, K, V) = softmax( (Q * K^T) / sqrt(d_k) ) * V\n",
                "```\n",
                "Where:\n",
                "*   `Q` is the matrix of queries.\n",
                "*   `K` is the matrix of keys.\n",
                "*   `V` is the matrix of values.\n",
                "*   `K^T` is the transpose of the key matrix.\n",
                "*   `d_k` is the dimension of the keys (and queries).\n",
                "*   `sqrt(d_k)` is the scaling factor used to prevent the dot products from becoming too large, which could push the softmax function into regions with very small gradients.\n",
                "*   `softmax` is applied row-wise to the scaled scores to obtain attention weights that sum to 1.\n",
                "\n",
                "Let's implement this step-by-step."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn.functional as F\n",
                "import math"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dummy data\n",
                "seq_len = 5    # Length of the input sequence\n",
                "embed_dim = 8  # Dimension of embeddings / hidden states (d_k)\n",
                "batch_size = 1 # Number of sequences processed in parallel\n",
                "\n",
                "# Create Query, Key, Value tensors\n",
                "# Shape: (batch_size, sequence_length, embedding_dimension)\n",
                "Q = torch.randn(batch_size, seq_len, embed_dim)\n",
                "K = torch.randn(batch_size, seq_len, embed_dim)\n",
                "V = torch.randn(batch_size, seq_len, embed_dim)\n",
                "\n",
                "print(\"Query (Q):\", Q.shape)\n",
                "print(\"Key (K):\", K.shape)\n",
                "print(\"Value (V):\", V.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 1 & 2: Calculate Scaled Scores\n",
                "\n",
                "First, we compute the dot product between each query and all keys (`Q * K^T`). Then, we scale these scores by dividing by the square root of the key dimension (`d_k`)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Calculate dot products between Query and Key (transposed)\n",
                "# We need K transposed so the dimensions match for matrix multiplication:\n",
                "# Q: (batch, seq_len, embed_dim)\n",
                "# K^T: (batch, embed_dim, seq_len)\n",
                "# Result (scores): (batch, seq_len, seq_len)\n",
                "# scores[b, i, j] represents the similarity between the i-th query and j-th key in batch b.\n",
                "scores = torch.matmul(Q, K.transpose(-2, -1))\n",
                "print(\"Raw Scores (Q * K^T):\", scores.shape)\n",
                "# print(scores)\n",
                "\n",
                "# 2. Scale the scores\n",
                "# Divide by the square root of the key dimension (d_k)\n",
                "d_k = K.size(-1) # embed_dim\n",
                "scaled_scores = scores / math.sqrt(d_k)\n",
                "print(\"\\nScaled Scores:\", scaled_scores.shape)\n",
                "# print(scaled_scores)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 3: Apply Softmax\n",
                "\n",
                "Apply the softmax function to the scaled scores along the key dimension (the last dimension of the `scaled_scores` tensor). This converts the scores into probabilities (attention weights) that sum to 1 for each query."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Apply Softmax to get attention weights\n",
                "# Softmax is applied across the last dimension (keys)\n",
                "# Result shape: (batch_size, seq_len, seq_len)\n",
                "# attention_weights[b, i, j] is the weight given to the j-th value vector when computing the output for the i-th query.\n",
                "attention_weights = F.softmax(scaled_scores, dim=-1)\n",
                "print(\"Attention Weights (Softmax):\", attention_weights.shape)\n",
                "# print(attention_weights)\n",
                "# print(\"Sum of weights for first query:\", attention_weights[0, 0, :].sum()) # Should be close to 1"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 4: Multiply Weights by Values\n",
                "\n",
                "Finally, multiply the attention weights by the Value matrix (`V`). This produces the context vector (the output of the attention layer), which is a weighted sum of the values, where the weights are determined by the query-key similarities."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Multiply weights by Value\n",
                "# attention_weights: (batch, seq_len, seq_len)\n",
                "# V: (batch, seq_len, embed_dim)\n",
                "# Result (context_vector): (batch, seq_len, embed_dim)\n",
                "# context_vector[b, i, :] is the output for the i-th query, computed as a weighted sum of all value vectors.\n",
                "context_vector = torch.matmul(attention_weights, V)\n",
                "print(\"Context Vector (Weights * V):\", context_vector.shape)\n",
                "# print(context_vector)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "The `context_vector` is the output of the Scaled Dot-Product Attention mechanism. It has the same shape as the Query and Value inputs. Each vector in the `context_vector` sequence (e.g., `context_vector[0, i, :]`) represents the information aggregated from the entire Value sequence (`V`), weighted according to how relevant each part of the input (represented by `K`) was to the corresponding query (`Q[0, i, :]`).\n",
                "\n",
                "This allows the model to focus on the most pertinent parts of the input sequence when generating each part of the output sequence."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example: Putting it all together in a function\n",
                "\n",
                "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
                "    \"\"\"Calculate scaled dot product attention.\n",
                "    \n",
                "    Args:\n",
                "        Q (torch.Tensor): Queries. Shape: (batch_size, ..., seq_len_q, d_k).\n",
                "        K (torch.Tensor): Keys. Shape: (batch_size, ..., seq_len_k, d_k).\n",
                "        V (torch.Tensor): Values. Shape: (batch_size, ..., seq_len_v, d_v).\n",
                "                          Note: seq_len_k == seq_len_v\n",
                "        mask (torch.Tensor, optional): Mask to apply before softmax. \n",
                "                                     Shape: (batch_size, ..., seq_len_q, seq_len_k).\n",
                "                                     Defaults to None.\n",
                "                                     \n",
                "    Returns:\n",
                "        torch.Tensor: Context vector. Shape: (batch_size, ..., seq_len_q, d_v).\n",
                "        torch.Tensor: Attention weights. Shape: (batch_size, ..., seq_len_q, seq_len_k).\n",
                "    \"\"\"\n",
                "    d_k = K.size(-1)\n",
                "    # Matmul Q and K^T: (..., seq_len_q, d_k) x (..., d_k, seq_len_k) -> (..., seq_len_q, seq_len_k)\n",
                "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
                "    \n",
                "    if mask is not None:\n",
                "        # Apply mask (typically -infinity for positions to ignore)\n",
                "        scores = scores.masked_fill(mask == 0, -1e9) \n",
                "        \n",
                "    # Apply softmax: (..., seq_len_q, seq_len_k)\n",
                "    attention_weights = F.softmax(scores, dim=-1)\n",
                "    \n",
                "    # Matmul weights and V: (..., seq_len_q, seq_len_k) x (..., seq_len_v, d_v) -> (..., seq_len_q, d_v)\n",
                "    # Note: seq_len_k == seq_len_v\n",
                "    context = torch.matmul(attention_weights, V)\n",
                "    \n",
                "    return context, attention_weights\n",
                "\n",
                "# --- Test the function ---\n",
                "context_output, weights_output = scaled_dot_product_attention(Q, K, V)\n",
                "\n",
                "print(\"\\n--- Function Output ---\")\n",
                "print(\"Context Vector Shape:\", context_output.shape)\n",
                "print(\"Attention Weights Shape:\", weights_output.shape)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}