{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "import torch_geometric\n",
    "import torch\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from torchmetrics.classification import BinaryConfusionMatrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from util.preprocess import findSensorActuator\n",
    "\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"USING {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "     col_1  col_2  attack  col_3  col_4  col_5  col_6\n",
      "0       19     40       0     87      1     57     34\n",
      "1       86     40       0     81     34     42     45\n",
      "2       66     94       1     61     80     54     23\n",
      "3       76     83       1     31     16     93     99\n",
      "4       32     86       0     46     96     12     13\n",
      "..     ...    ...     ...    ...    ...    ...    ...\n",
      "995     74     89       1     21     30     43     68\n",
      "996      7      3       0     68     60     69     33\n",
      "997     87     92       1     19      7      5     72\n",
      "998     63     43       0     86     37     71     28\n",
      "999     45     93       0     52     56      9     61\n",
      "\n",
      "[1000 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Number of rows in the DataFrame\n",
    "samples = 1000\n",
    "features=6\n",
    "# Creating random data for the first two columns\n",
    "col1 = np.random.randint(1, 100, samples)\n",
    "col2 = np.random.randint(1, 100, samples)\n",
    "\n",
    "# Define the mathematical relation for the 'Attack' column\n",
    "attack = np.where(col1 + col2 > 150, 1, 0)\n",
    "\n",
    "# Creating the DataFrame with six columns\n",
    "df = pd.DataFrame({\n",
    "    'col_1': col1,\n",
    "    'col_2': col2,\n",
    "    'attack': attack,  # Renaming Affair to Attack\n",
    "}|{f\"col_{i+1}\":np.random.randint(1, 100, samples) for i in range(2,features)})\n",
    "\n",
    "# Apply Min-Max Scaling, excluding the 'Attack' column\n",
    "# columns_to_scale = df.columns.difference(['attack'])\n",
    "# df_scaled = df.copy()\n",
    "# df_scaled[columns_to_scale] = (df[columns_to_scale] - df[columns_to_scale].min()) / (df[columns_to_scale].max() - df[columns_to_scale].min())\n",
    "\n",
    "# Display the original and scaled DataFrames\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "# print(\"\\nScaled DataFrame (with 'Attack' excluded from scaling):\")\n",
    "# print(df_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"./data/batadal/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mfindSensorActuator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\thesis\\GDN\\src\\util\\preprocess.py:133\u001b[39m, in \u001b[36mfindSensorActuator\u001b[39m\u001b[34m(dataFrame, ignor_labels)\u001b[39m\n\u001b[32m    129\u001b[39m columns = [\n\u001b[32m    130\u001b[39m     col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m dataFrame.columns \u001b[38;5;28;01mif\u001b[39;00m col.strip() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mdatetime\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mTimestamp\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    131\u001b[39m ]\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m     l = \u001b[38;5;28mlen\u001b[39m(\u001b[43mdataFrame\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m l == \u001b[32m2\u001b[39m:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m ignor_labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\moji\\miniconda3\\envs\\cuda_12_4\\Lib\\site-packages\\pandas\\core\\series.py:2407\u001b[39m, in \u001b[36mSeries.unique\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2344\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34munique\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> ArrayLike:  \u001b[38;5;66;03m# pylint: disable=useless-parent-delegation\u001b[39;00m\n\u001b[32m   2345\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2346\u001b[39m \u001b[33;03m    Return unique values of Series object.\u001b[39;00m\n\u001b[32m   2347\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2405\u001b[39m \u001b[33;03m    Categories (3, object): ['a' < 'b' < 'c']\u001b[39;00m\n\u001b[32m   2406\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2407\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\moji\\miniconda3\\envs\\cuda_12_4\\Lib\\site-packages\\pandas\\core\\base.py:1025\u001b[39m, in \u001b[36mIndexOpsMixin.unique\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1023\u001b[39m     result = values.unique()\n\u001b[32m   1024\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1025\u001b[39m     result = \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43munique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\moji\\miniconda3\\envs\\cuda_12_4\\Lib\\site-packages\\pandas\\core\\algorithms.py:401\u001b[39m, in \u001b[36munique\u001b[39m\u001b[34m(values)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34munique\u001b[39m(values):\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    Return unique values based on a hash table.\u001b[39;00m\n\u001b[32m    310\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    399\u001b[39m \u001b[33;03m    array([('a', 'b'), ('b', 'a'), ('a', 'c')], dtype=object)\u001b[39;00m\n\u001b[32m    400\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43munique_with_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\moji\\miniconda3\\envs\\cuda_12_4\\Lib\\site-packages\\pandas\\core\\algorithms.py:440\u001b[39m, in \u001b[36munique_with_mask\u001b[39m\u001b[34m(values, mask)\u001b[39m\n\u001b[32m    438\u001b[39m table = hashtable(\u001b[38;5;28mlen\u001b[39m(values))\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     uniques = \u001b[43mtable\u001b[49m\u001b[43m.\u001b[49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m     uniques = _reconstruct_data(uniques, original.dtype, original)\n\u001b[32m    442\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m uniques\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "findSensorActuator(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"docstring for MyDataset.\"\"\"\n",
    "    def __init__(self, df:pd.DataFrame,device=\"cuda\"):\n",
    "        super(MyDataset, self).__init__()\n",
    "        self.df=df.__deepcopy__()\n",
    "        self.labels=self.df[\"attack\"].__deepcopy__()\n",
    "        self.df.drop(columns=[\"attack\"],inplace=True)\n",
    "        self.device=device\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.df.iloc[index].to_numpy(),device=self.device).float()\\\n",
    "    ,torch.tensor([self.labels.iloc[index]],device=self.device).float()\n",
    "    \n",
    "# Create a Dataset\n",
    "dataset=MyDataset(df,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(train:pd.DataFrame, test:pd.DataFrame,excludes=[\"attack\"]):\n",
    "    cols=[col for col in train.columns if col not in excludes]\n",
    "    normalizer = MinMaxScaler(feature_range=(0, 1)).fit(train[cols]) # scale training data to [0,1] range\n",
    "    train[cols] = normalizer.transform(train[cols])\n",
    "    test[cols] = normalizer.transform(test[cols])\n",
    "    train=train.reindex()\n",
    "    test=test.reindex()\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and test sets\n",
    "train_df, test_df = train_test_split(df,test_size=.2,random_state=42)\n",
    "train_df, test_df=norm(train_df, test_df)\n",
    "train_loader=torch.utils.data.DataLoader(dataset=MyDataset(train_df),batch_size=16)    \n",
    "test_loader=torch.utils.data.DataLoader(dataset=MyDataset(test_df),batch_size=16)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from laf_model import LAFLayer\n",
    "class GCNLafConv(torch_geometric.nn.GCNConv):\n",
    "    def __init__(self,  units=1, node_dim=32, **kwargs):\n",
    "        super(GCNLafConv, self).__init__( **kwargs)\n",
    "        self.laf = LAFLayer(units=units, kernel_initializer='random_uniform')\n",
    "        self.mlp = torch.nn.Linear(node_dim*units, node_dim)\n",
    "        self.dim = node_dim\n",
    "        self.units = units\n",
    "    \n",
    "    def aggregate(self, inputs, index,**kwargs):\n",
    "        x = torch.sigmoid(inputs)\n",
    "        x = self.laf(x, index)\n",
    "        x = x.view((-1, self.dim * self.units))\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class CustomModule(torch.nn.Module):\n",
    "    def __init__(self, input_features, hidden_size,device=\"cuda\",edge_index=None):\n",
    "        \"\"\"\n",
    "        Initialize the custom PyTorch module with adj1, embedding layer, and soil tensor.\n",
    "\n",
    "        Args:\n",
    "            input_features (int): The size of the square matrix (adj1) and embedding size.\n",
    "            hidden_size (int): The size of the hidden dimension for the soil tensor.\n",
    "        \"\"\"\n",
    "        super(CustomModule, self).__init__()\n",
    "        self.input_features=input_features\n",
    "        self.hidden_size=hidden_size\n",
    "        # Define adj1 as a square matrix parameter\n",
    "        self.linear_transformations = torch.nn.ModuleList([\n",
    "            torch.nn.Linear(1, hidden_size) for _ in range(input_features)\n",
    "        ])\n",
    "        if edge_index is None:\n",
    "            G = nx.complete_graph(input_features)\n",
    "            self.adj1=torch.tensor([[[x,y] for y in l.keys() ] for x,l in G.adjacency()]).reshape(-1,2).T.reshape(2,-1).to(device)\n",
    "            self.adj1_weigthts = torch.nn.Parameter(torch.ones(self.adj1.shape[-1],requires_grad=True))\n",
    "        else:\n",
    "            self.adj1=torch.tensor(edge_index).to(device)\n",
    "            self.adj1_weigthts=None\n",
    "        # Define an embedding layer\n",
    "        # self.embedding = torch.nn.Embedding(in\n",
    "        self.lin1=torch.nn.Linear(hidden_size,1)\n",
    "        # put_features, hidden_size)\n",
    "        self.gcn1=torch_geometric.nn.GCNConv(hidden_size,1,add_self_loops=False)\n",
    "        self.lin1=torch.nn.Linear(input_features,1)\n",
    "        \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass to compute the embedding and add the soil tensor.\n",
    "\n",
    "        Args:\n",
    "            indices (torch.Tensor): Input tensor with indices for the embedding layer.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Resulting tensor after embedding and addition.\n",
    "        \"\"\"\n",
    "        shape=x.shape\n",
    "        if x.dim()==1:\n",
    "            x.unsqueeze_(0)\n",
    "        embedded_columns = [\n",
    "            self.linear_transformations[i](x[:,i].unsqueeze(-1)) for i in range(self.input_features)\n",
    "        ]\n",
    "        x=torch.cat(embedded_columns, dim=-1).reshape(*shape,self.hidden_size)\n",
    "        x=self.gcn1(x,self.adj1,self.adj1_weigthts)\n",
    "        x.squeeze_(-1)\n",
    "        x=torch.nn.functional.relu(x)\n",
    "        x=self.lin1(x)\n",
    "        x=torch.nn.functional.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def prop(self):\n",
    "        \"\"\"\n",
    "        Property to return adj1.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The adj1 parameter (square matrix).\n",
    "        \"\"\"\n",
    "        return self.adj1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=CustomModule(features,10,edge_index=[[2,3],[4,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=50\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=.001)\n",
    "loss_fn=torch.nn.BCELoss(reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:19<00:00,  1.59s/it, loss=0.282]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[174,   0],\n",
       "        [ 26,   0]], device='cuda:0')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=model.train().to(device=device)\n",
    "t=tqdm(torch.arange(epoch),postfix=\"loss\")\n",
    "with torch.autograd.set_detect_anomaly(False):\n",
    "    for e in t:\n",
    "        acu_loss=0\n",
    "        for i,(x,y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad(set_to_none=True)   \n",
    "            y_pred=model(x)\n",
    "            loss=loss_fn(y_pred,y)\n",
    "            acu_loss+=loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        t.set_postfix({\"loss\":(acu_loss/dataset.__len__())})\n",
    "\n",
    "\n",
    "conf=BinaryConfusionMatrix().to(device=device)\n",
    "\n",
    "model=model.eval()\n",
    "\n",
    "for i,(x,y) in enumerate(test_loader):\n",
    "    y_pred=model(x)\n",
    "    conf.update(y_pred,y)\n",
    "\n",
    "conf.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[110]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m adj=\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43madj1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m w=model.adj1_weigthts.detach().numpy()\n\u001b[32m      4\u001b[39m result_df=pd.DataFrame({\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mweigth\u001b[39m\u001b[33m\"\u001b[39m:w,\u001b[33m\"\u001b[39m\u001b[33mx\u001b[39m\u001b[33m\"\u001b[39m:adj[\u001b[32m0\u001b[39m],\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m:adj[\u001b[32m1\u001b[39m]\n\u001b[32m      6\u001b[39m })\n",
      "\u001b[31mTypeError\u001b[39m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "adj=model.adj1.detach().numpy()\n",
    "w=model.adj1_weigthts.detach().numpy()\n",
    "\n",
    "result_df=pd.DataFrame({\n",
    "    \"weigth\":w,\"x\":adj[0],\"y\":adj[1]\n",
    "})\n",
    "result_df.sort_values(by=\"weigth\",ascending=False).head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_12_4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
