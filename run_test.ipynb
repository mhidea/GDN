{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from parameters.params import Params,Datasets,Models,Tasks\n",
    "from util.env import set_param\n",
    "from main import Main\n",
    "import torch\n",
    "from models.mine.MSTGAT import MSTGAT\n",
    "from test_loop import test\n",
    "from train_loop import train\n",
    "import pandas as pd\n",
    "from evaluate import IqrThreshold,MyConfusuion,IqrSensorThreshold,MinMaxThreshold,ZscoreThreshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'DatasetLoader' on <module 'util.consts' from 'd:\\\\thesis\\\\GDN\\\\src\\\\util\\\\consts.py'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m path = \u001b[33m\"\u001b[39m\u001b[33m./snapshot/my_mstgat_batadal/25_05_31_07_36_22/10/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m param: Params = \u001b[43mpickle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpath\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43mparam.pickle\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m param.summary()\n",
      "\u001b[31mAttributeError\u001b[39m: Can't get attribute 'DatasetLoader' on <module 'util.consts' from 'd:\\\\thesis\\\\GDN\\\\src\\\\util\\\\consts.py'>"
     ]
    }
   ],
   "source": [
    "path = \"./snapshot/my_mstgat_batadal/25_05_31_07_36_22/10/\"\n",
    "\n",
    "param: Params = pickle.load(file=open(f\"{path}param.pickle\", \"rb\"))\n",
    "param.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param.val_ratio=0\n",
    "set_param(param)\n",
    "print(param.task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters={}\n",
    "if param.model in [Models.my_mstgat,Models.my_mstgat2]:\n",
    "    model_parameters = {\"gamma1\": 0.5, \"gamma2\": 0.8, \"kernel_size\": 16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main = Main(param=param,modelParams=model_parameters)\n",
    "# main.load_leastTrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(main.train_dataloader),len(main.val_dataloader),len(main.test_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train losses\n",
    "get all losses from train data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_loss,_,_ = test(main.model,main.train_dataloader,None)\n",
    "train_all_loss.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_all_losses,ys,lbls = test(main.model, main.test_dataloader,confusion=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IqrThreshold\n",
    "From all losses get the threshold And confusion metrics.\n",
    "Based onthe threshold and confusion, get the real output of test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr=IqrSensorThreshold()\n",
    "thr.fit(train_all_loss)\n",
    "conf=MyConfusuion(thr=thr).to(device=param.device)\n",
    "print(\"test_all_losses.shape: \",test_all_losses.shape)\n",
    "conf.update(test_all_losses,lbls)\n",
    "confusion_matrix=conf.compute()\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinMaxThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mmthr=MinMaxThreshold(multiplier=.95)\n",
    "mmthr.fit(train_all_loss)\n",
    "preds=mmthr.transform(test_all_losses)\n",
    "mmConf=MyConfusuion(thr=mmthr).cuda()\n",
    "mmConf.update(test_all_losses,lbls)\n",
    "mmConf.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft=pd.DataFrame({\"FN\":[],\"FP\":[],\"m\":[]})\n",
    "for m in torch.arange(start=.92,step=.005,end=1.2):\n",
    "    mxthr=MinMaxThreshold(multiplier=m)\n",
    "    mxthr.fit(train_all_loss)\n",
    "    mpreds=mxthr.transform(test_all_losses)\n",
    "    mxConf=MyConfusuion(thr=mxthr).cuda()\n",
    "    mxConf.update(test_all_losses,lbls)\n",
    "    mtr=mxConf.compute().cpu().numpy()\n",
    "    dft.loc[len(dft)]=[mtr[0][1],mtr[1][0],m.numpy()]\n",
    "dft.plot(x=\"m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmthr=ZscoreThreshold(threshold=3.42)\n",
    "mmthr.fit(train_all_loss)\n",
    "preds=mmthr.transform(test_all_losses)\n",
    "mmConf=MyConfusuion(thr=mmthr).cuda()\n",
    "mmConf.update(test_all_losses,lbls)\n",
    "mmConf.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft=pd.DataFrame({\"FN\":[],\"FP\":[],\"m\":[]})\n",
    "for m in torch.arange(start=.8,step=.01,end=3.8):\n",
    "    mxthr=ZscoreThreshold(threshold=m)\n",
    "    mxthr.fit(train_all_loss)\n",
    "    mpreds=mxthr.transform(test_all_losses)\n",
    "    mxConf=MyConfusuion(thr=mxthr).cuda()\n",
    "    mxConf.update(test_all_losses,lbls)\n",
    "    mtr=mxConf.compute().cpu().numpy()\n",
    "    dft.loc[len(dft)]=[mtr[0][1],mtr[1][0],m.numpy()]\n",
    "dft.plot(x=\"m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AbsMaxThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import AbsMaxThreshold\n",
    "mmthr=AbsMaxThreshold(multiplier=3.7)\n",
    "mmthr.fit(train_all_loss)\n",
    "preds=mmthr.transform(test_all_losses)\n",
    "mmConf=MyConfusuion(thr=mmthr).cuda()\n",
    "mmConf.update(test_all_losses,lbls)\n",
    "mmConf.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dft=pd.DataFrame({\"FN\":[],\"FP\":[]})\n",
    "for m in torch.arange(start=.5,step=.1,end=5):\n",
    "    mxthr=AbsMaxThreshold(multiplier=m)\n",
    "    mxthr.fit(train_all_loss)\n",
    "    mpreds=mxthr.transform(test_all_losses)\n",
    "    mxConf=MyConfusuion(thr=mxthr).cuda()\n",
    "    mxConf.update(test_all_losses,lbls)\n",
    "    mtr=mxConf.compute().cpu().numpy()\n",
    "    dft.loc[len(dft)]=[mtr[0][1],mtr[1][0]]\n",
    "dft.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmthr=IqrSensorThreshold()\n",
    "# mmthr=MinMaxThreshold(multiplier=.995)\n",
    "mmthr.fit(train_all_loss)\n",
    "preds=mmthr.transform(test_all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOTTING\n",
    "\n",
    "\n",
    "all test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=pd.DataFrame({\"preds\":preds.tolist(),\"truth\":lbls.tolist()})\n",
    "df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each attack zone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from util.data import get_attack_interval,getAttacks\n",
    "\n",
    "\n",
    "attacks=getAttacks(df,\"truth\")\n",
    "# Identify contiguous regions of ones\n",
    "\n",
    "# Create subplots dynamically\n",
    "num_regions = len(attacks)\n",
    "fig, axes = plt.subplots(num_regions, 1, figsize=(8, 3*num_regions), sharex=False)\n",
    "\n",
    "if num_regions == 1:\n",
    "    axes = [axes]  # Ensure axes is iterable\n",
    "\n",
    "for i in range(num_regions):\n",
    "    # first_idx = group.index.min()  # First occurrence of 1\n",
    "    # last_idx = group.index.max()   # Last occurrence of 1\n",
    "    [first_idx,last_idx]=attacks[i]\n",
    "    range_before = first_idx - int(1.0 *(last_idx-first_idx))\n",
    "    range_after = last_idx + int(1.0 *(last_idx-first_idx))\n",
    "    if range_before<0:\n",
    "        range_before=0\n",
    "    if range_after>len(df):\n",
    "        range_after=last_idx\n",
    "    # df.iloc[first_idx:last_idx].plot(ax=ax)\n",
    "    axes[i].plot( df.loc[range_before:range_after,'truth'], label=\"Truth\")\n",
    "    axes[i].plot( df.loc[range_before:range_after,'preds'], color='red', label=\"Preds\",linestyle=\"--\")\n",
    "\n",
    "    # ax.set_xlim(range_before, range_after)  # X-axis limit dynamically adjusted\n",
    "    # ax.set(ybound=(0, max(df['preds']) + 10))  # Corrected method for Y-axis limits\n",
    "\n",
    "    axes[i].set_title(f\"Attack {i+1}\")\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True)\n",
    "\n",
    "plt.xlabel(\"Index\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"max: \",test_result[0].max(),\n",
    "    #   val_result[0].max(),\n",
    "      train_result[0].max())\n",
    "print(\"min: \",test_result[0].min(),\n",
    "    #   val_result[0].min(),\n",
    "      train_result[0].min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.data import getAttacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum=0#val_result[0].min()\n",
    "maximum=train_result[0].max().item()*1#*24.5\n",
    "\n",
    "pred=torch.where((test_result[0]>maximum) ,torch.tensor(1),torch.tensor(0)).cpu().numpy()\n",
    "labels_df=pd.DataFrame({\"pred\":pred,\"truth\":test_result[2].cpu().numpy()})\n",
    "attacks=getAttacks(labels_df,\"truth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i,attack in enumerate(attacks):\n",
    "    left=(attack[0]//100)*100\n",
    "    right=((attack[1]//100)+1)*100\n",
    "    labels_df.loc[left:right].plot(title=f\"attack {i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=test_result[3].sum(-1)\n",
    "print(r.max(),r.min(),r.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import createMetrics\n",
    "k=createMetrics(test_result,maximum)\n",
    "print(k)\n",
    "k.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k2=['TP', 'FP', 'TN', 'FN']\n",
    "k1=[\"Accuracy\",\"Precision\",\"Recall\",\"F1\"]\n",
    "d1={key:[] for key in k1}\n",
    "d2={key:[] for key in k2}\n",
    "x=[]\n",
    "for i in range(100):\n",
    "    _max=maximum*(.5+i/100)\n",
    "    rr=createMetrics(test_result,_max)\n",
    "    x.append(_max)\n",
    "    for key  in d1.keys():\n",
    "        d1[key].append(float(rr[key]))\n",
    "    for key  in d2.keys():\n",
    "        d2[key].append(float(rr[key]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_12_4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
