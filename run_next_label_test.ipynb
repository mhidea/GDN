{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from util.params import Params\n",
    "from util.env import set_param\n",
    "from main import Main\n",
    "import torch\n",
    "from models.GDN import GDN\n",
    "from test_loop import test\n",
    "from train_loop import train\n",
    "import pandas as pd\n",
    "from util.data import getAttacks\n",
    "from evaluate import createMetrics, createStats\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy\n",
    "from ipywidgets import Checkbox, interact, FloatSlider, fixed\n",
    "\n",
    "\n",
    "def plot_with_full_row(\n",
    "    thr=False,\n",
    "    pred=False,\n",
    "    threshold=1.0,\n",
    "    stat: dict = None,\n",
    "    score: numpy.ndarray = None,\n",
    "    labels: numpy.ndarray = None,\n",
    "    run:int=0\n",
    "):\n",
    "    minimum = 0  # val_result[0].min()\n",
    "    maximum = stat[\"threshold\"].cpu().clone().numpy() * threshold  # *24.5\n",
    "    pred_filtered = numpy.where((score > maximum), 1, 0)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"thr\": numpy.ones_like(pred_filtered) * maximum,\n",
    "            \"pred\": score,\n",
    "            \"pred_filterd\": pred_filtered,\n",
    "            \"truth\": labels,\n",
    "        }\n",
    "    )\n",
    "    attacks = getAttacks(df, \"truth\")\n",
    "    num_plots = len(attacks)  # Number of data points\n",
    "    num_rows = (num_plots + 1) // 2  # Two columns per row\n",
    "\n",
    "    # Create a figure with a grid layout, including space for the full-row plot\n",
    "    fig = plt.figure(figsize=(12, 6 + 5 * num_rows))  # Adjust overall size\n",
    "    grid = fig.add_gridspec(num_rows + 1, 2)  # Add one row for the overarching plot\n",
    "\n",
    "    # Full-row plot at the top\n",
    "    ax_full_row = fig.add_subplot(grid[0, :])  # Span across all columns\n",
    "    df.plot(ax=ax_full_row)\n",
    "    ax_full_row.set_title(f\"all attacks at  {run}\")\n",
    "    ax_full_row.legend()\n",
    "    cols = df.columns\n",
    "    if not thr:\n",
    "        cols = [col for col in cols if col != \"thr\"]\n",
    "    if not pred:\n",
    "        cols = [col for col in cols if col != \"pred\"]\n",
    "    max = df.shape[0]\n",
    "    # Individual subplots below\n",
    "    for i, attack in enumerate(attacks):\n",
    "        leng = attack[1] - attack[0]\n",
    "        left = attack[0] - leng\n",
    "        right = attack[1] + leng\n",
    "        right = right if right < max else max\n",
    "        left = left if left > 0 else 0\n",
    "        row, col = divmod(i, 2)  # Determine row and column for the subplot\n",
    "        ax = fig.add_subplot(grid[row + 1, col])  # Offset by one for the full-row plot\n",
    "        df.loc[left:right, cols].plot(ax=ax)  # Example plot\n",
    "        ax.set_title(f\"attack {i+1}\")\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path=\"./snapshot/gnn_tam_swat/25_04_11_17_15_19\"\n",
    "count=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# DATASET \n",
      "\n",
      "*Datasets.swat*\n",
      "Model is trained. Loading from file .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score:\n",
      " 1.1863654 220.41959 54.86868\n",
      "FINISHED\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "labels = None\n",
    "stats = []\n",
    "labels=[]\n",
    "for run in range(count):\n",
    "    path = f\"{main_path}/{run}/\"\n",
    "    if not os.path.exists(f\"{path}param.pickle\"):\n",
    "        print(f\"Skipping run {run}\")\n",
    "        continue\n",
    "    param: Params = pickle.load(file=open(f\"{path}param.pickle\", \"rb\"))\n",
    "    main.model.cpu()\n",
    "    mode_dict = pickle.load(file=open(f\"{path}model_parameters.pickle\", \"rb\"))\n",
    "    main.model.cuda()\n",
    "    param.val_ratio = 0\n",
    "    set_param(param)\n",
    "    # print(param.summary())\n",
    "    main = Main(param=param, modelParams=mode_dict)\n",
    "    # main.model.load_state_dict(pickle.load(open(main.param.best_path().replace(\"best.pt\",\"best_train.pt\"),\"rb\")), weights_only=True)\n",
    "    _, _, label = next(\n",
    "            iter(\n",
    "                torch.utils.data.DataLoader(\n",
    "                    main.test_dataset,\n",
    "                    batch_size=main.test_dataset.__len__(),\n",
    "                    shuffle=False,\n",
    "                    pin_memory=False,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    labels.append(label.cpu().squeeze(-1).numpy())\n",
    "    # print(len(main.train_dataloader),len(main.val_dataloader),len(main.test_dataloader))\n",
    "    train_losses = test(main.model, main.train_dataloader)\n",
    "    stat = createStats(train_losses)\n",
    "    # print(stat)\n",
    "    # print(\"train_avg_loss: \",train_avg_loss)\n",
    "    # val_avg_loss, val_result = test(main.model, main.val_dataloader)\n",
    "    # val_avg_loss\n",
    "    test_losses = test(main.model, main.test_dataloader)\n",
    "\n",
    "    # print(\"test_avg_loss: \",test_avg_loss)\n",
    "    # print(\"max: \",test_result[0].max(),\n",
    "    #     #   val_result[0].max(),\n",
    "    #       train_result[0].max())\n",
    "    # print(\"min: \",test_result[0].min(),\n",
    "    #     #   val_result[0].min(),\n",
    "    #       train_result[0].min())\n",
    "    score = ((test_losses - stat[\"medians\"]).abs() / stat[\"iqr\"]).max(-1).values\n",
    "    score=score.cpu().clone().numpy()\n",
    "    print(\"score:\\n\", score.min(), score.max(), score.mean())\n",
    "    scores.append(score)\n",
    "    stats.append(stat)\n",
    "\n",
    "print(\"FINISHED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "910dee92ba464453998afa176e7959fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Checkbox(value=True, description='thr'), Checkbox(value=True, description='pred'), Floatâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for run in range(count):\n",
    "    interact(\n",
    "        plot_with_full_row,\n",
    "        thr=Checkbox(value=True, description=\"thr\"),\n",
    "        pred=Checkbox(value=True, description=\"pred\"),\n",
    "        threshold=FloatSlider(value=1.0,min=0.5,max=2.0,step=.01,description=\"threshold\"),\n",
    "        score=fixed(scores[run]),\n",
    "        stat=fixed(stats[run]),\n",
    "        labels=fixed(labels[run]),\n",
    "        run=fixed(run)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.classification import BinaryStatScores\n",
    "\n",
    "selected_run=2\n",
    "threshold=.91\n",
    "for value in [1,threshold]:\n",
    "    \n",
    "    new_threshold=stats[selected_run][\"threshold\"].cpu().numpy()*value\n",
    "    pred=numpy.where(scores[selected_run]>new_threshold,1,0)\n",
    "    bc=BinaryStatScores()\n",
    "    bc.update(torch.tensor(pred),torch.tensor(labels[selected_run]))\n",
    "    cfm=bc.compute()\n",
    "    print(createMetrics(cfm))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_12_4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
