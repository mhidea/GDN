{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from util.params import Params\n",
    "from util.env import set_param\n",
    "from main import Main\n",
    "import torch\n",
    "from models.GDN import GDN\n",
    "from test_loop import test\n",
    "from train_loop import train\n",
    "import pandas as pd\n",
    "from util.data import getAttacks\n",
    "from evaluate import createMetrics\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "def plot_with_full_row(df:pd.DataFrame,attacks,run):\n",
    "    num_plots = len(attacks)  # Number of data points\n",
    "    num_rows = (num_plots + 1) // 2  # Two columns per row\n",
    "\n",
    "    # Create a figure with a grid layout, including space for the full-row plot\n",
    "    fig = plt.figure(figsize=(12, 6 + 5 * num_rows))  # Adjust overall size\n",
    "    grid = fig.add_gridspec(num_rows + 1, 2)  # Add one row for the overarching plot\n",
    "\n",
    "    # Full-row plot at the top\n",
    "    ax_full_row = fig.add_subplot(grid[0, :])  # Span across all columns\n",
    "    df.plot(ax=ax_full_row)\n",
    "    ax_full_row.set_title(f\"all attacks at run: {run}\")\n",
    "    ax_full_row.legend()\n",
    "\n",
    "    # Individual subplots below\n",
    "    for i, attack in enumerate(attacks):\n",
    "        left=(attack[0]//100)*100\n",
    "        right=((attack[1]//100)+1)*100\n",
    "        row, col = divmod(i, 2)  # Determine row and column for the subplot\n",
    "        ax = fig.add_subplot(grid[row + 1, col])  # Offset by one for the full-row plot\n",
    "        df.loc[left:right].plot(ax=ax)  # Example plot\n",
    "        ax.set_title(f\"attack {i+1}\")\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path=\"./snapshot/gdn_swat/25_04_04_21_13_20\"\n",
    "count=1\n",
    "thr=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# DATASET \n",
      "\n",
      "*Datasets.swat*\n",
      "#####################################\n",
      "sensors count:  31\n",
      "actuators count:  9\n",
      "consts count:  12\n",
      "consts:  {'P102': 1.0, 'P201': 1.0, 'P202': 1.0, 'P204': 1.0, 'P206': 1.0, 'P401': 1.0, 'P403': 1.0, 'P404': 1.0, 'P502': 1.0, 'P601': 1.0, 'P603': 1.0, 'attack': 0.0}\n",
      "#####################################\n",
      "['FIT101', 'LIT101', 'MV101', 'AIT201', 'AIT202', 'AIT203', 'FIT201', 'MV201', 'DPIT301', 'FIT301', 'LIT301', 'MV301', 'MV302', 'MV303', 'MV304', 'AIT401', 'AIT402', 'FIT401', 'LIT401', 'AIT501', 'AIT502', 'AIT503', 'AIT504', 'FIT501', 'FIT502', 'FIT503', 'FIT504', 'PIT501', 'PIT502', 'PIT503', 'FIT601']\n",
      "Model is trained. Loading from file .....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m main = Main(param=param,modelParams=mode_dict)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# print(len(main.train_dataloader),len(main.val_dataloader),len(main.test_dataloader))\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m train_avg_loss, train_result = test(main.model, main.train_dataloader)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# print(\"train_avg_loss: \",train_avg_loss)\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# val_avg_loss, val_result = test(main.model, main.val_dataloader)\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# val_avg_loss\u001b[39;00m\n\u001b[32m     18\u001b[39m test_avg_loss, test_result = test(\n\u001b[32m     19\u001b[39m             main.model, main.test_dataloader\n\u001b[32m     20\u001b[39m         )\n",
      "\u001b[31mValueError\u001b[39m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "for run in range(count):\n",
    "  path = f\"{main_path}/{run}/\"\n",
    "  if not os.path.exists(f\"{path}param.pickle\"):\n",
    "    print(f\"Skipping run {run}\")\n",
    "    continue\n",
    "  param: Params = pickle.load(file=open(f\"{path}param.pickle\", \"rb\"))\n",
    "  mode_dict=pickle.load(file=open(f\"{path}model_parameters.pickle\", \"rb\"))\n",
    "  param.val_ratio=0\n",
    "  set_param(param)\n",
    "  # print(param.summary())\n",
    "  main = Main(param=param,modelParams=mode_dict)\n",
    "\n",
    "  # print(len(main.train_dataloader),len(main.val_dataloader),len(main.test_dataloader))\n",
    "  train_avg_loss, train_result = test(main.model, main.train_dataloader)\n",
    "  # print(\"train_avg_loss: \",train_avg_loss)\n",
    "  # val_avg_loss, val_result = test(main.model, main.val_dataloader)\n",
    "  # val_avg_loss\n",
    "  test_avg_loss, test_result = test(\n",
    "              main.model, main.test_dataloader\n",
    "          )\n",
    "  # print(\"test_avg_loss: \",test_avg_loss)\n",
    "  # print(\"max: \",test_result[0].max(),\n",
    "  #     #   val_result[0].max(),\n",
    "  #       train_result[0].max())\n",
    "  # print(\"min: \",test_result[0].min(),\n",
    "  #     #   val_result[0].min(),\n",
    "  #       train_result[0].min())\n",
    "\n",
    "  minimum=0#val_result[0].min()\n",
    "  maximum=train_result[0].max().item()*thr#*24.5\n",
    "\n",
    "  pred_normalized=torch.where((test_result[0]>maximum) ,torch.tensor(1),torch.tensor(0)).cpu().numpy()\n",
    "\n",
    "  labels_df=pd.DataFrame({\"pred\":test_result[0].cpu().numpy().squeeze(-1),\n",
    "                          \"pred_filterd\":pred_normalized.squeeze(-1),\n",
    "                          \"truth\":test_result[2].cpu().squeeze(-1).numpy()})\n",
    "  attacks=getAttacks(labels_df,\"truth\")\n",
    "  plot_with_full_row(labels_df,attacks,run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=createMetrics(test_result,maximum)\n",
    "print(k)\n",
    "k.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k2=['TP', 'FP', 'TN', 'FN']\n",
    "k1=[\"Accuracy\",\"Precision\",\"Recall\",\"F1\"]\n",
    "d1={key:[] for key in k1}\n",
    "d2={key:[] for key in k2}\n",
    "x=[]\n",
    "for i in range(100):\n",
    "    _max=maximum*(.5+i/100)\n",
    "    rr=createMetrics(test_result,_max)\n",
    "    x.append(_max)\n",
    "    for key  in d1.keys():\n",
    "        d1[key].append(float(rr[key]))\n",
    "    for key  in d2.keys():\n",
    "        d2[key].append(float(rr[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.DataFrame(d1,index=x)\n",
    "df1.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.DataFrame(d2,index=x)\n",
    "df2.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_12_4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
